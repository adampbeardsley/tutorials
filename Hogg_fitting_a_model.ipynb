{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "This is a notebook to work through David Hogg's \"Data analysis recipes: Fitting a model to data\" (https://arxiv.org/abs/1008.4686)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1 - Standard Practice\n",
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.array([203, 58, 210, 202, 198, 158, 165, 201, 157,\n",
    "              131, 166, 160, 186, 125, 218, 146]).astype(np.float64)\n",
    "y = np.array([495, 173, 479, 504, 510, 416, 393, 442, 317,\n",
    "              311, 400, 337, 423, 334, 533, 344]).astype(np.float64)\n",
    "sigma_y = np.array([21, 15, 27, 14, 30, 16, 14, 25, 52,\n",
    "                    16, 34, 31, 42, 26, 16, 22]).astype(np.float64)\n",
    "covar = np.diag(sigma_y**2.0)\n",
    "covar_inv = np.linalg.inv(covar)\n",
    "A = np.stack([np.ones_like(x), x]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "invmat = np.linalg.inv(np.dot(A.transpose(), np.dot(covar_inv, A)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bm = np.dot(invmat, np.dot(A.transpose(), np.dot(covar_inv, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print our answers for intercept (b) and slope (m)\n",
    "bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the data and the fitted line\n",
    "plt.errorbar(x, y, yerr=sigma_y, fmt='o')\n",
    "plt.plot(np.arange(300), bm[1] * np.arange(300) + bm[0])\n",
    "plt.xlim([0, 300]); plt.ylim([0, 700]);\n",
    "plt.xlabel('x'); plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# invmat is the covariance matrix of our fit parameters.\n",
    "# So the uncertainty variance, sigma^2_m, on the slope\n",
    "# is the (1, 1) element of this matrix.\n",
    "invmat[1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Add more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([201, 244, 47, 287, 203, 58, 210, 202, 198, 158,\n",
    "              165, 201, 157, 131, 166, 160, 186, 125, 218, 146]).astype(np.float64)\n",
    "y = np.array([592, 401, 583, 402, 495, 173, 479, 504, 510,\n",
    "              416, 393, 442, 317, 311, 400, 337, 423, 334, 533, 344]).astype(np.float64)\n",
    "sigma_y = np.array([61, 25, 38, 15, 21, 15, 27, 14, 30, 16,\n",
    "                    14, 25, 52, 16, 34, 31, 42, 26, 16, 22]).astype(np.float64)\n",
    "covar = np.diag(sigma_y**2.0)\n",
    "covar_inv = np.linalg.inv(covar)\n",
    "A = np.stack([np.ones_like(x), x]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "invmat = np.linalg.inv(np.dot(A.transpose(), np.dot(covar_inv, A)))\n",
    "bm = np.dot(invmat, np.dot(A.transpose(), np.dot(covar_inv, y)))\n",
    "print(bm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the data and the fitted line\n",
    "plt.errorbar(x, y, yerr=sigma_y, fmt='o')\n",
    "plt.plot(np.arange(300), bm[1] * np.arange(300) + bm[0])\n",
    "plt.xlim([0, 300]); plt.ylim([0, 700]);\n",
    "plt.xlabel('x'); plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Uncertainty variance on slope:\n",
    "invmat[1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what we see here is that the new data points (four of them) seem to\n",
    "not agree with the other 16 points, causing the data to look more scattered.\n",
    "However, they have very small error bars, and so the uncertainty on the\n",
    "fit dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Generalize to quadratic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([203, 58, 210, 202, 198, 158, 165, 201, 157,\n",
    "              131, 166, 160, 186, 125, 218, 146]).astype(np.float64)\n",
    "y = np.array([495, 173, 479, 504, 510, 416, 393, 442, 317,\n",
    "              311, 400, 337, 423, 334, 533, 344]).astype(np.float64)\n",
    "sigma_y = np.array([21, 15, 27, 14, 30, 16, 14, 25, 52,\n",
    "                    16, 34, 31, 42, 26, 16, 22]).astype(np.float64)\n",
    "covar = np.diag(sigma_y**2.0)\n",
    "covar_inv = np.linalg.inv(covar)\n",
    "A = np.stack([np.ones_like(x), x, x**2.0]).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "invmat = np.linalg.inv(np.dot(A.transpose(), np.dot(covar_inv, A)))\n",
    "bmq = np.dot(invmat, np.dot(A.transpose(), np.dot(covar_inv, y)))\n",
    "print(bmq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the data and the fitted line\n",
    "plt.errorbar(x, y, yerr=sigma_y, fmt='o')\n",
    "plt.plot(np.arange(300), bmq[2] * np.arange(300)**2.0 + bmq[1] * np.arange(300) + bmq[0])\n",
    "plt.xlim([0, 300]); plt.ylim([0, 700]);\n",
    "plt.xlabel('x'); plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Section 1 - The Objective Function\n",
    "### Exercise 4\n",
    "\n",
    "Each of our data points can be described with the frequency distribution\n",
    "$$p(t_i|\\sigma_{ti}, T) = \\frac{1}{\\sqrt{2\\pi \\sigma_{ti}^2}} \\mathrm{exp}\\left(-\\frac{[t_i - T]^2}{2\\sigma_{ti}^2}\\right) $$\n",
    "And the likelihood is the product of these probabilities.\n",
    "$$\\mathcal{L} = \\prod_{i=1}^{N} p(t_i|\\sigma_{ti}, T)$$\n",
    "The log likelihood is then\n",
    "$$\\ln\\mathcal{L} = K - \\sum_{i=1}^N \\frac{[t_i - T]^2}{2\\sigma_{ti}^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then take a derivative of $\\ln \\mathcal{L}$ with respect to $T$ and set equal to zero to maximize the likelihood.\n",
    "$$ \\frac{\\partial \\ln \\mathcal{L}}{\\partial T} = \\sum_{i=1}^N \\frac{t_i-T}{\\sigma_{ti}^2} = 0$$\n",
    "$$ \\sum_{i=1}^N \\frac{t_i}{\\sigma_{ti}^2} = \\sum_{i=1}^N \\frac{T}{\\sigma_{ti}^2} $$\n",
    "$$ \\sum_{i=1}^N \\frac{t_i}{\\sigma_{ti}^2} = T \\sum_{i=1}^N \\frac{1}{\\sigma_{ti}^2} $$\n",
    "Dividing by the sum on the right hand side, we arrive at our result, which is simply a weighted mean, where the weights are given by the inverse of the variances.\n",
    "$$ \\frac{\\sum_{i=1}^N \\frac{t_i}{\\sigma_{ti}^2}}{\\sum_{i=1}^N \\frac{1}{\\sigma_{ti}^2} } = T $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "Start with the matrix expression for $\\chi^2$.\n",
    "$$ \\chi^2 = [\\mathbf{Y} - \\mathbf{A X}]^T \\mathbf{C}^{-1} [\\mathbf{Y} - \\mathbf{A X}]$$\n",
    "In order to more clearly see how the matrix multiplications, transforms, etc work, I will use Einstein notation. Repeated indices are summed, and unpaired indices are the indices of the total expression. I will also drop the boldface just to save on my own typing.\n",
    "$$ \\chi^2 = [Y_i - A_{ij} X_j] C^{-1}_{ik} [Y_k - A_{kl} X_l] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we take a derivative with respect to one component of $X$. I will use yet another index, say $a$. Now, because $a$ will be unpaired, the result will actually be a vector, and we can read off each component of $X$ through $a$. Note that even though $X_a$ does not explicitly appear in our expression for $\\chi^2$, $X_j$ and $X_l$ will both assume the value of $X_a$ within their sums. As a result, taking a derivative will pick up the terms when $j=a$ and when $l=a$. After employing the produce rule of derivatives, we end up with two terms (which are really more than two terms because they each contain sums).\n",
    "$$\\frac{\\partial \\chi^2}{\\partial X_a} = -A_{ia}C^{-1}_{ik}[Y_k-A_{kl}X_l] - [Y_i-A_{ij}X_j]C^{-1}_{ik}A_{ka} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will see the beauty of using Einstein notation to handle our matrix operations. Even though matrix multiplications do not in general commute, in this form we can readily move terms around as long as we do not break any summation rules. So I will rearrange the second term to look like the first.\n",
    "$$ -A_{ia}C^{-1}_{ik}[Y_k-A_{kl}X_l] - [Y_i-A_{ij}X_j]C^{-1}_{ik}A_{ka} =  -A_{ia}C^{-1}_{ik}[Y_k-A_{kl}X_l] - A_{ka}C^{-1}_{ik}[Y_i-A_{ij}X_j]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, using the wonderfulness of Einstein notation, we switch around some of the \"dummy indices\" - indices which are summed over, so they do not carry any meaning outside their own term. In the first term, I will replace $k$ with $i$, $l$ with $j$, and $i$ with $k$.\n",
    "$$ -A_{ia}C^{-1}_{ik}[Y_k-A_{kl}X_l] - A_{ka}C^{-1}_{ik}[Y_i-A_{ij}X_j] = -A_{ka}C^{-1}_{ki}[Y_i-A_{ij}X_j] - A_{ka}C^{-1}_{ik}[Y_i-A_{ij}X_j]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we note that $C^{-1}$ is symmetric, so we can exchange its indices freely. This gets us half way to our result.\n",
    "$$ -A_{ka}C^{-1}_{ki}[Y_i-A_{ij}X_j] - A_{ka}C^{-1}_{ik}[Y_i-A_{ij}X_j] = -A_{ka}C^{-1}_{ki}[Y_i-A_{ij}X_j] - A_{ka}C^{-1}_{ki}[Y_i-A_{ij}X_j] $$\n",
    "$$ = -2 A_{ka}C^{-1}_{ki}[Y_i-A_{ij}X_j] $$\n",
    "And setting our derivative equal to zero,\n",
    "$$ \\frac{\\partial \\chi^2}{\\partial X_a} = -2 A_{ka}C^{-1}_{ki}[Y_i-A_{ij}X_j] = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage (and it takes practice to know when to do this), it is convenient to switch back to matrix notation. But I'm still not going to use boldface.\n",
    "$$ A^T C^{-1} [Y - AX] = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining steps are to distribute the sum, move one term to the RHS, and insert some matrix inverses to solve for $X$.\n",
    "$$ A^TC^{-1}Y - A^T C^{-1} A X = 0 $$\n",
    "$$ A^T C^{-1} Y = A^T C^{-1} A X $$\n",
    "$$ [A^T C^{-1} A]^{-1} [A^T C^{-1} Y] = [A^T C^{-1} A]^{-1} A^T C^{-1} A X $$\n",
    "$$ [A^T C^{-1} A]^{-1} [A^T C^{-1} Y] = X $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the result we started wtih, Eq. 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tutorials]",
   "language": "python",
   "name": "conda-env-tutorials-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
